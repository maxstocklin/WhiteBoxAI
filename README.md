# WhiteBoxAI
## Advanced Explainable AI Platform

A powerful Streamlit-based tool designed to explore and interpret predictions made by an XGBoost model. Built for transparency, auditability, and accessibility to both technical and non-technical users.

---

## ğŸ“¦ Features


### ğŸ§ª Train / Test / Visualize
This page provides an overview of model performance and global explainability:
- Classification Report: Shows precision, recall, F1-score, and support per class.
- Confusion Matrix: Visual breakdown of true/false positives and negatives.
- ROC Curve & AUC: Measures model discrimination power.
- Feature Correlation Matrix:
  - Toggle between raw features or SHAP values.
  - Understand relationships between features and the label.
- SHAP Summary Plot:
  - Bar chart of feature importance using SHAP values.
- SHAP Force Plot:
  - Visual breakdown of a single prediction.
- Interaction Explorer:
  - Analyze and interpret feature Ã— feature interactions using SHAP interaction values.
  - Includes LLM-generated descriptions per pair.


### ğŸ“„ Dataset Viewer
- Full interactive table with test samples (decoded features)
- Rich filtering:
  - ğŸ” By label or feature range
  - Separate numeric and categorical filters
- Instant link to interpretation per row
- Built with `st.dataframe` + clickable links for intuitive navigation


### ğŸ”¬ Sample Interpretation
- Confidence Report:
  - Evaluates every prediction across **6+ trustworthiness metrics**:
    - ğŸ”® Prediction Probability
    - ğŸ“¦ Category Familiarity
    - ğŸŒ Distance to Training Samples
    - ğŸ§¨ Z-Score Anomalies
    - ğŸ§  Tree Class Consensus (Top-K Trees)
    - ğŸŒ Tree Path Similarity
    - ğŸŒ² Rare Path Coverage
  - Aggregates into an overall confidence level (ğŸŸ¢ High, ğŸŸ¡ Medium, ğŸ”´ Low)
- Feature-Level Explanation: 
  - Full breakdown of model decision logic per feature
- LLM Assistant Assessment:
  - Natural language interpretation generated using Mistral
  - Summarizes what influenced the decision and why it makes sense
- Bottom sticky bar: summarizes true vs predicted label and confidence, with animated color transitions when label flips


### ğŸ›ï¸ Prediction Simulator
- Compare original vs. adjusted predictions side-by-side
- Live edit any feature value â€” including categorical and numerical fields
- Bottom sticky bar summarizes prediction shift with animated color indicator
- Three main views:
  - ğŸ” **Counterfactual Explanations** (powered by DiCE): What minimal change would flip the outcome?
  - ğŸšï¸ **Adjust Sample Inputs**: Manual controls for real-time feature tweaking
  - ğŸ” **Top SHAP Differences**: Compare original vs. modified prediction explanations


### âš–ï¸ Fairness Dashboard
- **Data Integrity Checks**
  - Detects missing values, placeholder symbols like `'?'`, and class imbalance.
- **Fairness Metrics**
  - Select a sensitive feature like `sex` or `race`
  - Analyze how model performance and SHAP attributions vary across groups
  - Explore fairness concerns like disparate impact or demographic parity

---

## ğŸš€ Setup & Installation

```bash
# Clone the repo
git clone https://github.com/maxstocklin/interpretableML.git
cd interpretableML/xgb-interpretability-app

# Install dependencies
pip install -r requirements.txt

# Run the app
streamlit run Home.py
# Note: LLM features require Apple Silicon (Mac) and will not run on Streamlit Cloud

# Optional: Enable counterfactuals via DiCE
pip install -r dice_patch.txt
```

Ensure you have the dataset in `data/adult.data`.

---

## ğŸ¤– LLM Integration

Uses [mlx-community/Mistral-7B-Instruct-v0.2-4bit] locally with:
- Custom sampling (temperature, top-k, top-p)
- Streaming responses
- Caching of responses for each sample in `/logs/explanations/`
> âš ï¸ **Note:** LLM-based explanations use `mlx` and `mlx-lm`, which currently require a Mac with Apple Silicon (M1/M2/M3) and cannot run on Streamlit Cloud. On deployed versions, LLM features will be disabled with a friendly warning.

---

## ğŸ› ï¸ Planned Extensions

We're continuously improving the app to enhance interpretability, transparency, and trust. Here's what's on the roadmap:

- ğŸ¤– RAG Q&A Assistant
  - Let users ask questions like â€œWhat matters most for this profile?â€, or â€œWhat changed the outcome?â€
- ğŸ§± Audit Toolkit
  - Export full explanations (SHAP, tree paths, confidence metrics) for any prediction, with visual + JSON traceability.
- ğŸ“Š Data Drift Detection
  - Compare incoming data distributions to training data â€” flag anomalies, unseen categories, and suspicious shifts.
- ğŸ” MLOps Integration
  - Support model versioning, prediction logging, monitoring (e.g. confidence drops), and seamless retraining hooks.

---

## ğŸ§© Optional: DiCE Patch for Counterfactuals

To enable counterfactual explanations with `dice-ml`, you need a special patch due to its older dependency on `pandas<2.0.0`.

Use this helper file:

```bash
pip install -r dice_patch.txt
```

This file applies:
- `pandas<2.0.0`
- `dice-ml==0.11`
- `pandas==2.2.3` (reinstated after install)

---

## ğŸ¤ Contributing

1. Fork the repo
2. Create a feature branch
3. Submit a PR with a clear explanation

---

## ğŸ§  [Demo](https://whitebox.streamlit.app)

https://whitebox.streamlit.app
